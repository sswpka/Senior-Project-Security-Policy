{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import shap\n",
    "import os\n",
    "import plotly.graph_objects as go\n",
    "from shap.plots import waterfall, beeswarm\n",
    "from shap import Explanation, KernelExplainer\n",
    "import numpy as np\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, r2_score\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generic policy : project_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Beebie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.6890894175553732\n",
      "Accuracy: 0.6147540983606558\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.66      0.60        53\n",
      "           1       0.69      0.58      0.63        69\n",
      "\n",
      "    accuracy                           0.61       122\n",
      "   macro avg       0.62      0.62      0.61       122\n",
      "weighted avg       0.63      0.61      0.62       122\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "X_train = pd.read_csv(\"auto/Generic policy_project_feature_X_train.csv\")\n",
    "X_test = pd.read_csv(\"auto/Generic policy_project_feature_X_test.csv\")\n",
    "y_train = pd.read_csv(\"auto/Generic policy_project_feature_y_train.csv\")\n",
    "y_test = pd.read_csv(\"auto/Generic policy_project_feature_y_test.csv\")\n",
    "\n",
    "# Train model\n",
    "model = RandomForestClassifier(\n",
    "    max_features=1, min_samples_leaf=6, min_samples_split=7,\n",
    "    n_estimators=512, n_jobs=1, random_state=1, warm_start=True)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_probs = model.predict_proba(X_test)[:, 1]\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"AUC:\", roc_auc_score(y_test, y_probs))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generic policy : security_practice_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.7094155844155844\n",
      "Accuracy: 0.6557377049180327\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.47      0.60        66\n",
      "           1       0.58      0.88      0.70        56\n",
      "\n",
      "    accuracy                           0.66       122\n",
      "   macro avg       0.70      0.67      0.65       122\n",
      "weighted avg       0.71      0.66      0.64       122\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train = pd.read_csv(\"auto/Generic policy_security_practice_X_train.csv\")\n",
    "X_test = pd.read_csv(\"auto/Generic policy_security_practice_X_test.csv\")\n",
    "y_train = pd.read_csv(\"auto/Generic policy_security_practice_y_train.csv\").values.ravel()\n",
    "y_test = pd.read_csv(\"auto/Generic policy_security_practice_y_test.csv\").values.ravel()\n",
    "\n",
    "model = RandomForestClassifier(\n",
    "    bootstrap=False, max_features=1, min_samples_leaf=3,\n",
    "    min_samples_split=14, n_estimators=512, n_jobs=1,\n",
    "    random_state=1, warm_start=True)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_probs = model.predict_proba(X_test)[:, 1]\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"AUC:\", roc_auc_score(y_test, y_probs))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generic policy : code_quality_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.5687655343827671\n",
      "Accuracy: 0.5901639344262295\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.62      0.64        71\n",
      "           1       0.51      0.55      0.53        51\n",
      "\n",
      "    accuracy                           0.59       122\n",
      "   macro avg       0.58      0.58      0.58       122\n",
      "weighted avg       0.60      0.59      0.59       122\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train = pd.read_csv(\"auto/Generic policy_project_quality_X_train.csv\")\n",
    "X_test = pd.read_csv(\"auto/Generic policy_project_quality_X_test.csv\")\n",
    "y_train = pd.read_csv(\"auto/Generic policy_project_quality_y_train.csv\").values.ravel()\n",
    "y_test = pd.read_csv(\"auto/Generic policy_project_quality_y_test.csv\").values.ravel()\n",
    "\n",
    "model = AdaBoostClassifier(algorithm='SAMME',\n",
    "    estimator=DecisionTreeClassifier(max_depth=10),\n",
    "    learning_rate=0.010381491760996881, n_estimators=362,\n",
    "    random_state=1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_probs = model.predict_proba(X_test)[:, 1]\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"AUC:\", roc_auc_score(y_test, y_probs))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reporting mechanism : project_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.8991419783066213\n",
      "Accuracy: 0.8164556962025317\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.82      0.80        71\n",
      "           1       0.85      0.82      0.83        87\n",
      "\n",
      "    accuracy                           0.82       158\n",
      "   macro avg       0.81      0.82      0.82       158\n",
      "weighted avg       0.82      0.82      0.82       158\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train = pd.read_csv(\"auto/Reporting mechanism_project_feature_X_train.csv\")\n",
    "X_test = pd.read_csv(\"auto/Reporting mechanism_project_feature_X_test.csv\")\n",
    "y_train = pd.read_csv(\"auto/Reporting mechanism_project_feature_y_train.csv\").values.ravel()\n",
    "y_test = pd.read_csv(\"auto/Reporting mechanism_project_feature_y_test.csv\").values.ravel()\n",
    "\n",
    "model = ExtraTreesClassifier(bootstrap=True, criterion='entropy', max_features=1,\n",
    "                             min_samples_split=7, n_estimators=512, n_jobs=1,\n",
    "                             random_state=1, warm_start=True)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_probs = model.predict_proba(X_test)[:, 1]\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"AUC:\", roc_auc_score(y_test, y_probs))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reporting mechanism : security_practice_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.8322916666666667\n",
      "Accuracy: 0.7911392405063291\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.72      0.78        80\n",
      "           1       0.75      0.86      0.80        78\n",
      "\n",
      "    accuracy                           0.79       158\n",
      "   macro avg       0.80      0.79      0.79       158\n",
      "weighted avg       0.80      0.79      0.79       158\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train = pd.read_csv(\"auto/Reporting mechanism_security_practice_X_train.csv\")\n",
    "X_test = pd.read_csv(\"auto/Reporting mechanism_security_practice_X_test.csv\")\n",
    "y_train = pd.read_csv(\"auto/Reporting mechanism_security_practice_y_train.csv\").values.ravel()\n",
    "y_test = pd.read_csv(\"auto/Reporting mechanism_security_practice_y_test.csv\").values.ravel()\n",
    "\n",
    "model = HistGradientBoostingClassifier(early_stopping=True,\n",
    "                                       l2_regularization=8.9e-5,\n",
    "                                       learning_rate=0.1991,\n",
    "                                       max_iter=512, max_leaf_nodes=955,\n",
    "                                       min_samples_leaf=33,\n",
    "                                       n_iter_no_change=2,\n",
    "                                       random_state=1,\n",
    "                                       validation_fraction=None,\n",
    "                                       warm_start=True)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_probs = model.predict_proba(X_test)[:, 1]\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"AUC:\", roc_auc_score(y_test, y_probs))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reporting mechanism : code_quality_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.8352573352573353\n",
      "Accuracy: 0.8037974683544303\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.83      0.81        77\n",
      "           1       0.83      0.78      0.80        81\n",
      "\n",
      "    accuracy                           0.80       158\n",
      "   macro avg       0.80      0.80      0.80       158\n",
      "weighted avg       0.81      0.80      0.80       158\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train = pd.read_csv(\"auto/Reporting mechanism_project_quality_X_train.csv\")\n",
    "X_test = pd.read_csv(\"auto/Reporting mechanism_project_quality_X_test.csv\")\n",
    "y_train = pd.read_csv(\"auto/Reporting mechanism_project_quality_y_train.csv\").values.ravel()\n",
    "y_test = pd.read_csv(\"auto/Reporting mechanism_project_quality_y_test.csv\").values.ravel()\n",
    "\n",
    "model = KNeighborsClassifier(n_neighbors=2, p=1, weights='distance')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_probs = model.predict_proba(X_test)[:, 1]\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"AUC:\", roc_auc_score(y_test, y_probs))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scope of practice : project_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.5614545454545454\n",
      "Accuracy: 0.5238095238095238\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.44      0.49        55\n",
      "           1       0.50      0.62      0.55        50\n",
      "\n",
      "    accuracy                           0.52       105\n",
      "   macro avg       0.53      0.53      0.52       105\n",
      "weighted avg       0.53      0.52      0.52       105\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train = pd.read_csv(\"auto/Scope of practice_project_feature_X_train.csv\")\n",
    "X_test = pd.read_csv(\"auto/Scope of practice_project_feature_X_test.csv\")\n",
    "y_train = pd.read_csv(\"auto/Scope of practice_project_feature_y_train.csv\").values.ravel()\n",
    "y_test = pd.read_csv(\"auto/Scope of practice_project_feature_y_test.csv\").values.ravel()\n",
    "\n",
    "model = RandomForestClassifier(max_features=15, min_samples_leaf=5,\n",
    "                               min_samples_split=20, n_estimators=512,\n",
    "                               n_jobs=1, random_state=1, warm_start=True)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_probs = model.predict_proba(X_test)[:, 1]\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"AUC:\", roc_auc_score(y_test, y_probs))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scope of practice : security_practice_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.4858181818181818\n",
      "Accuracy: 0.5238095238095238\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.44      0.47        50\n",
      "           1       0.54      0.60      0.57        55\n",
      "\n",
      "    accuracy                           0.52       105\n",
      "   macro avg       0.52      0.52      0.52       105\n",
      "weighted avg       0.52      0.52      0.52       105\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Beebie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train = pd.read_csv(\"auto/Scope of practice_security_practice_X_train.csv\")\n",
    "X_test = pd.read_csv(\"auto/Scope of practice_security_practice_X_test.csv\")\n",
    "y_train = pd.read_csv(\"auto/Scope of practice_security_practice_y_train.csv\").values.ravel()\n",
    "y_test = pd.read_csv(\"auto/Scope of practice_security_practice_y_test.csv\").values.ravel()\n",
    "\n",
    "model = AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=9),\n",
    "                           learning_rate=1.97, n_estimators=101,\n",
    "                           random_state=1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_probs = model.predict_proba(X_test)[:, 1]\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"AUC:\", roc_auc_score(y_test, y_probs))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scope of practice : code_quality_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.6028874269005848\n",
      "Accuracy: 0.5619047619047619\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.62      0.57        48\n",
      "           1       0.62      0.51      0.56        57\n",
      "\n",
      "    accuracy                           0.56       105\n",
      "   macro avg       0.57      0.57      0.56       105\n",
      "weighted avg       0.57      0.56      0.56       105\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train = pd.read_csv(\"auto/Scope of practice_project_quality_X_train.csv\")\n",
    "X_test = pd.read_csv(\"auto/Scope of practice_project_quality_X_test.csv\")\n",
    "y_train = pd.read_csv(\"auto/Scope of practice_project_quality_y_train.csv\").values.ravel()\n",
    "y_test = pd.read_csv(\"auto/Scope of practice_project_quality_y_test.csv\").values.ravel()\n",
    "\n",
    "model = RandomForestClassifier(bootstrap=False, criterion='entropy', max_features=4,\n",
    "                               min_samples_leaf=7, n_estimators=512, n_jobs=1,\n",
    "                               random_state=1, warm_start=True)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_probs = model.predict_proba(X_test)[:, 1]\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"AUC:\", roc_auc_score(y_test, y_probs))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User guideline : project_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9497599451303155\n",
      "Accuracy: 0.8496732026143791\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.78      0.83        72\n",
      "           1       0.82      0.91      0.87        81\n",
      "\n",
      "    accuracy                           0.85       153\n",
      "   macro avg       0.86      0.85      0.85       153\n",
      "weighted avg       0.85      0.85      0.85       153\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train = pd.read_csv(\"auto/User guideline_project_feature_X_train.csv\")\n",
    "X_test = pd.read_csv(\"auto/User guideline_project_feature_X_test.csv\")\n",
    "y_train = pd.read_csv(\"auto/User guideline_project_feature_y_train.csv\").values.ravel()\n",
    "y_test = pd.read_csv(\"auto/User guideline_project_feature_y_test.csv\").values.ravel()\n",
    "\n",
    "model = ExtraTreesClassifier(criterion='entropy', max_features=2,\n",
    "                             n_estimators=512, n_jobs=1,\n",
    "                             random_state=1, warm_start=True)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_probs = model.predict_proba(X_test)[:, 1]\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"AUC:\", roc_auc_score(y_test, y_probs))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User guideline : security_practice_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.8057074504442926\n",
      "Accuracy: 0.738562091503268\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.84      0.76        77\n",
      "           1       0.80      0.63      0.71        76\n",
      "\n",
      "    accuracy                           0.74       153\n",
      "   macro avg       0.75      0.74      0.74       153\n",
      "weighted avg       0.75      0.74      0.74       153\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train = pd.read_csv(\"auto/User guideline_security_practice_X_train.csv\")\n",
    "X_test = pd.read_csv(\"auto/User guideline_security_practice_X_test.csv\")\n",
    "y_train = pd.read_csv(\"auto/User guideline_security_practice_y_train.csv\").values.ravel()\n",
    "y_test = pd.read_csv(\"auto/User guideline_security_practice_y_test.csv\").values.ravel()\n",
    "\n",
    "model = HistGradientBoostingClassifier(early_stopping=True,\n",
    "                                       l2_regularization=0.1144,\n",
    "                                       learning_rate=0.3565,\n",
    "                                       max_iter=128, max_leaf_nodes=570,\n",
    "                                       min_samples_leaf=52,\n",
    "                                       n_iter_no_change=20,\n",
    "                                       random_state=1,\n",
    "                                       validation_fraction=0.2674,\n",
    "                                       warm_start=True)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_probs = model.predict_proba(X_test)[:, 1]\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"AUC:\", roc_auc_score(y_test, y_probs))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User guideline : code_quality_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.8623971193415638\n",
      "Accuracy: 0.7843137254901961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Beebie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.89      0.81        81\n",
      "           1       0.84      0.67      0.74        72\n",
      "\n",
      "    accuracy                           0.78       153\n",
      "   macro avg       0.80      0.78      0.78       153\n",
      "weighted avg       0.79      0.78      0.78       153\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train = pd.read_csv(\"auto/User guideline_project_quality_X_train.csv\")\n",
    "X_test = pd.read_csv(\"auto/User guideline_project_quality_X_test.csv\")\n",
    "y_train = pd.read_csv(\"auto/User guideline_project_quality_y_train.csv\").values.ravel()\n",
    "y_test = pd.read_csv(\"auto/User guideline_project_quality_y_test.csv\").values.ravel()\n",
    "\n",
    "model = AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=2),\n",
    "                           learning_rate=0.1688, n_estimators=144,\n",
    "                           random_state=1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_probs = model.predict_proba(X_test)[:, 1]\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"AUC:\", roc_auc_score(y_test, y_probs))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TreeExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_type == \"tree\":\n",
    "    #### Tree explainer ####\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values_tree = explainer.shap_values(X_test)\n",
    "\n",
    "    #### TMean and Absolute ####\n",
    "    shap_df = pd.DataFrame(shap_values_tree[:, :, 1], columns=X_test.columns)\n",
    "\n",
    "    # Compute and print raw mean SHAP values\n",
    "    abs_mean_shap_raw = shap_df.abs().mean(0).sort_values(ascending=False)\n",
    "    print(\"Absolute Mean SHAP raw:\")\n",
    "    print(abs_mean_shap_raw)\n",
    "    # Compute and print raw mean SHAP values\n",
    "    mean_shap_raw = shap_df.mean(0).sort_values(ascending=False)\n",
    "\n",
    "    #### prepare to plot ####\n",
    "    feature_names = X_test.columns.tolist() # Get feature names from X_test\n",
    "\n",
    "    num_samples, num_features, num_classes = shap_values_tree.shape  \n",
    "    reshaped_values = shap_values_tree.reshape(num_samples, num_features * num_classes)\n",
    "\n",
    "    columns = [f\"{feature_names[i]}_Class_{j}\" for i in range(num_features) for j in range(num_classes)] \n",
    "\n",
    "    df = pd.DataFrame(reshaped_values, columns=columns)\n",
    "\n",
    "    # Select only columns corresponding to Class 1\n",
    "    class_1_columns = [col for col in columns if \"_Class_1\" in col]  \n",
    "    df_class_1 = df[class_1_columns]  \n",
    "\n",
    "    df_class_1.columns = [col.replace(\"_Class_1\", \"\") for col in df_class_1.columns] \n",
    "\n",
    "    #### Plot ####\n",
    "    # Compute absolute mean SHAP values for correct sorting\n",
    "    df_mean = df_class_1.abs().mean(numeric_only=True).reset_index() \n",
    "    df_mean.columns = ['Feature', 'Mean |SHAP Value|']\n",
    "    df_mean = df_mean.sort_values(by=\"Mean |SHAP Value|\", ascending=True) \n",
    "\n",
    "    # Compute actual mean SHAP values for correct bar coloring\n",
    "    df_signed_mean = df_class_1.mean(numeric_only=True).reset_index()\n",
    "    df_signed_mean.columns = ['Feature', 'Mean SHAP Value']\n",
    "\n",
    "    # Merge both to get proper ordering and sign information\n",
    "    df_final = df_mean.merge(df_signed_mean, on=\"Feature\")\n",
    "\n",
    "    # Assign colors: red for positive, blue for negative values\n",
    "    df_final[\"Color\"] = df_final[\"Mean SHAP Value\"].apply(lambda x: \"mediumturquoise\" if x >= 0 else \"crimson\")\n",
    "\n",
    "    # Creating the horizontal bar plot with correct order\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=df_final[\"Mean |SHAP Value|\"],\n",
    "        y=df_final[\"Feature\"],\n",
    "        orientation='h',\n",
    "        marker=dict(color=df_final[\"Color\"]),\n",
    "        text=[f\"{v:.3f}\" for v in df_final[\"Mean |SHAP Value|\"]], \n",
    "        textposition=\"outside\"\n",
    "    ))\n",
    "\n",
    "    # Updating layout to match SHAP importance plot style\n",
    "    fig.update_layout(\n",
    "\n",
    "        title=f\"{cate}:{feature}\",\n",
    "        title_font_size=24,\n",
    "        xaxis_title=\"Mean SHAP Value (Average Impact on Model Output)\",\n",
    "        xaxis_title_font_size=20, \n",
    "        # yaxis_title=\"Features\",\n",
    "        # yaxis_title_font_size=21,\n",
    "        template=\"plotly_white\",\n",
    "        font=dict(size=18), \n",
    "        xaxis=dict(\n",
    "            range=model_range,\n",
    "            tickfont=dict(size=18),  \n",
    "            zeroline=True,  \n",
    "            zerolinecolor=\"black\",\n",
    "            zerolinewidth=2,\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            tickfont=dict(size=18) \n",
    "        ),\n",
    "        showlegend=False, \n",
    "        width=1200,  \n",
    "        height=500,  \n",
    "    )\n",
    "\n",
    "    # Show the plot\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tree: HistGradientBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_type == \"hist\":\n",
    "    #### Tree explainer ####\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values_tree = explainer.shap_values(X_test)\n",
    "\n",
    "    # Check if SHAP values are a list \n",
    "    if isinstance(shap_values_tree, list):  \n",
    "        shap_values_tree = shap_values_tree[1]  \n",
    "\n",
    "    # Ensure it's a NumPy array\n",
    "    shap_values_tree = np.array(shap_values_tree)\n",
    "\n",
    "    #### Mean and Absolute Mean SHAP values ####\n",
    "    shap_df = pd.DataFrame(shap_values_tree, columns=X_test.columns)\n",
    "\n",
    "    # Compute absolute mean SHAP values\n",
    "    abs_mean_shap_raw = shap_df.abs().mean(0).sort_values(ascending=False)\n",
    "\n",
    "    # Compute actual mean SHAP values\n",
    "    mean_shap_raw = shap_df.mean(0).sort_values(ascending=False)\n",
    "\n",
    "    #### Prepare Data for Plot ####\n",
    "    feature_names = X_test.columns.tolist()  # Get feature names from X_test\n",
    "\n",
    "    # Convert SHAP values to DataFrame\n",
    "    df_class_1 = pd.DataFrame(shap_values_tree, columns=feature_names)\n",
    "\n",
    "    #### Plot ####\n",
    "    # Compute absolute mean SHAP values for correct sorting\n",
    "    df_mean = df_class_1.abs().mean().reset_index()\n",
    "    df_mean.columns = ['Feature', 'Mean |SHAP Value|']\n",
    "    df_mean = df_mean.sort_values(by=\"Mean |SHAP Value|\", ascending=True)  \n",
    "\n",
    "    # Compute actual mean SHAP values for correct bar coloring\n",
    "    df_signed_mean = df_class_1.mean(numeric_only=True).reset_index()\n",
    "    df_signed_mean.columns = ['Feature', 'Mean SHAP Value']\n",
    "\n",
    "    # Merge both to get proper ordering and sign information\n",
    "    df_final = df_mean.merge(df_signed_mean, on=\"Feature\")\n",
    "\n",
    "    # Assign colors: red for positive, blue for negative values\n",
    "    df_final[\"Color\"] = df_final[\"Mean SHAP Value\"].apply(lambda x: \"mediumturquoise\" if x >= 0 else \"crimson\")\n",
    "\n",
    "    # Creating the horizontal bar plot with correct order\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=df_final[\"Mean |SHAP Value|\"],\n",
    "        y=df_final[\"Feature\"],\n",
    "        orientation='h',\n",
    "        marker=dict(color=df_final[\"Color\"]),\n",
    "        text=[f\"{v:.3f}\" for v in df_final[\"Mean |SHAP Value|\"]],  # Annotate absolute SHAP values\n",
    "        textposition=\"outside\"\n",
    "    ))\n",
    "\n",
    "    # Updating layout to match SHAP importance plot style\n",
    "    fig.update_layout(\n",
    "    title=f\"{cate}:{feature}\",\n",
    "    title_font_size=24,\n",
    "    xaxis_title=\"Mean SHAP Value (Average Impact on Model Output)\",\n",
    "    xaxis_title_font_size=20,\n",
    "    yaxis_title=\"Features\",\n",
    "    yaxis_title_font_size=21,\n",
    "    template=\"plotly_white\",\n",
    "    font=dict(size=18),\n",
    "    xaxis=dict(\n",
    "        range=model_range,  # Extend range to avoid label cutting\n",
    "        tickfont=dict(size=18),\n",
    "        zeroline=True,  # Keep central reference line\n",
    "        zerolinecolor=\"black\",\n",
    "        zerolinewidth=2,\n",
    "        automargin=False,\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        tickfont=dict(size=18),\n",
    "        automargin=True,  # Automatically adjust margins\n",
    "    ),\n",
    "    showlegend=False,\n",
    "    width=1400,  # Increase width to fit labels better\n",
    "    height=600,  # Increase height for better spacing\n",
    "    margin=dict(l=400, r=50, t=50, b=50),  # Extend left margin for long feature names\n",
    "    )\n",
    "\n",
    "    # Show the plot\n",
    "    fig.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KernelExplainer (Model-Agnostic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_type == \"kernel\" or model_type == \"ada\":\n",
    "    #### Kernel Explainer ####\n",
    "    background = X_train.sample(n=50, random_state=42)  \n",
    "    explainer = shap.KernelExplainer(model.predict, background)\n",
    "    shap_values_kernel = explainer.shap_values(X_test, nsamples=100)\n",
    "\n",
    "    # Ensure SHAP values are a NumPy array\n",
    "    shap_values_kernel = np.array(shap_values_kernel)\n",
    "\n",
    "    #### Mean and Absolute Mean SHAP values ####\n",
    "    shap_df = pd.DataFrame(shap_values_kernel, columns=X_test.columns)\n",
    "\n",
    "    # Compute absolute mean SHAP values\n",
    "    abs_mean_shap_raw = shap_df.abs().mean(0).sort_values(ascending=False)\n",
    "    print(\"Absolute Mean SHAP raw:\")\n",
    "    print(abs_mean_shap_raw)\n",
    "\n",
    "    # Compute actual mean SHAP values\n",
    "    mean_shap_raw = shap_df.mean(0).sort_values(ascending=False)\n",
    "\n",
    "    #### Prepare Data for Plot ####\n",
    "    feature_names = X_test.columns.tolist()  # Get feature names from X_test\n",
    "    df_class_1 = pd.DataFrame(shap_values_kernel, columns=feature_names)\n",
    "\n",
    "    #### Plot ####\n",
    "# Calculate the % of absolute SHAP values to use as a denominator for percentage calculations\n",
    "total_shap_sum = df_final['Adjusted SHAP Value'].abs().sum()\n",
    "\n",
    "# Calculate percentage of each SHAP value relative to the total\n",
    "df_final['Percentage SHAP Value'] = (df_final['Adjusted SHAP Value'] / total_shap_sum) \n",
    "\n",
    "# Print the updated dataframe to see the percentage values\n",
    "print(df_final[['Feature', 'Percentage SHAP Value']])\n",
    "\n",
    "# Creating the horizontal bar plot for percentage SHAP values\n",
    "fig_percentage = go.Figure()\n",
    "fig_percentage.add_trace(go.Bar(\n",
    "    x=df_final[\"Percentage SHAP Value\"],\n",
    "    y=df_final[\"Feature\"],\n",
    "    orientation='h',\n",
    "    marker=dict(color=df_final[\"Color\"]),  # Use the same color mapping as before\n",
    "    text=[f\"{v:.2f}\" for v in df_final[\"Percentage SHAP Value\"]],  # Annotate percentage SHAP values\n",
    "    textposition=\"outside\",\n",
    "    textfont=dict(size=34.5, color='black'),\n",
    "))\n",
    "\n",
    "# Updating layout to match SHAP importance plot style, adjusted for percentage values\n",
    "fig_percentage.update_layout(\n",
    "    title=f\"{cate} : {feature}\",  \n",
    "    title_font_size=48,\n",
    "    title_font=dict(color='black'),\n",
    "    xaxis_title=\"Weight of SHAP Value\",\n",
    "    xaxis_title_font_size=39,\n",
    "    template=\"plotly_white\",\n",
    "    xaxis=dict(\n",
    "        range=[-1.1, 1.1],\n",
    "        tickfont=dict(size=38),\n",
    "        zeroline=True,\n",
    "        zerolinecolor=\"black\",\n",
    "        zerolinewidth=2,\n",
    "        automargin=False,\n",
    "        color='black'\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        tickfont=dict(size=38),\n",
    "        automargin=True,\n",
    "        color='black'\n",
    "\n",
    "    ),\n",
    "    showlegend=False,\n",
    "    width=1420,\n",
    "    height=650,\n",
    "    margin=dict(l=450, r=30, t=70, b=120),\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig_percentage.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
